# Локальный RAG (Retrieval-Augmented Generation) и Агенты  


## 1. Установить Ollama
https://ollama.com/download/windows

см. подробнее в этой публикации: https://t.me/AiExp01/81

## 2. Установить llama3.2
ollama pull llama3.2:3b-instruct-fp16 

## 3. Проверяем доступность локальной модели

Запускать модуль `01_Simple_Request_Local_Model.py`

Ключевые моменты:
- Модуль использует `loguru` для управления логированием, включая сохранение логов в файлы с ротацией и сжатием 
по мере роста файла.
- Основная функция в модуле, `get_model_response`, получает тему запроса и возвращает ответ, сгенерированный 
языковой моделью.
- Использует мини-промпт в формате, позволяющим задавать контекст и особенности ответа модели, что позволяет 
контролировать длину и форму ответа.
- Программа импортирует и использует класс `ChatOllama` для работы с конкретной моделью LLM.

## 4. Простой RAG для pdf файлов
Поместить в папку Python\pdf один или несколько pdf фалов с текстовым слоем.

Запускать модуль `02_Simple_RAG_PDF.py`

Этот код импортирует необходимые модули, конфигурирует логирование, и инициализирует процесс обработки знаний 
через векторные поиски и языковую модель. 

Основной функционал состоит из трех функций: 
* `get_index_db()` для работы с векторной Базой-Знаний, 

* `get_message_content()` для извлечения релевантных данных,

* `get_model_response()` для формирования ответа от модели.

## 5. Локальный RAG с агентами на LLaMA3

Запускать модуль `03_Local_RAG_Agent.py`